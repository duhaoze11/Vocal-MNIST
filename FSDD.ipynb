{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 120\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 64, 64\n",
    "\n",
    "# the data, split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/home/jon/Documents/workspace/free-spoken-digit-dataset/spectrograms_all/\"\n",
    "file_names = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) and '.png' in f]\n",
    "lst = []\n",
    "y = []\n",
    "for f in file_names:\n",
    "    temp = plt.imread(dir_path+f, )\n",
    "    y.append(int(f[0]))\n",
    "    lst.append(temp)\n",
    "X = np.asarray(lst)\n",
    "Y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsttmp = []\n",
    "# for i in Y :\n",
    "#     tmp = []\n",
    "#     for numbers in range(10) :\n",
    "#         if numbers == i :\n",
    "#             num = 1\n",
    "#         else :\n",
    "#             num = 0\n",
    "#         tmp.append(num)\n",
    "#     lsttmp.append(tmp)\n",
    "# Y = np.asarray(lsttmp)\n",
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 5, ..., 5, 9, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 4, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 4, img_rows, img_cols)\n",
    "    input_shape = (4, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 4)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 4)\n",
    "    input_shape = (img_rows, img_cols, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 64, 64, 4)\n",
      "(1340, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1340, 64, 64, 4)\n",
      "1340 train samples\n",
      "660 test samples\n",
      "Train on 1206 samples, validate on 134 samples\n",
      "Epoch 1/120\n",
      "1206/1206 [==============================] - 2s 1ms/step - loss: 2.3027 - acc: 0.0920 - val_loss: 2.3025 - val_acc: 0.0970\n",
      "Epoch 2/120\n",
      "1206/1206 [==============================] - 0s 248us/step - loss: 2.3027 - acc: 0.0879 - val_loss: 2.3027 - val_acc: 0.0672\n",
      "Epoch 3/120\n",
      "1206/1206 [==============================] - 0s 246us/step - loss: 2.3024 - acc: 0.1103 - val_loss: 2.3027 - val_acc: 0.0672\n",
      "Epoch 4/120\n",
      "1206/1206 [==============================] - 0s 244us/step - loss: 2.3024 - acc: 0.0962 - val_loss: 2.3026 - val_acc: 0.0672\n",
      "Epoch 5/120\n",
      "1206/1206 [==============================] - 0s 246us/step - loss: 2.3023 - acc: 0.1061 - val_loss: 2.3027 - val_acc: 0.0970\n",
      "Epoch 6/120\n",
      "1206/1206 [==============================] - 0s 244us/step - loss: 2.3022 - acc: 0.0954 - val_loss: 2.3027 - val_acc: 0.0672\n",
      "Epoch 7/120\n",
      "1206/1206 [==============================] - 0s 246us/step - loss: 2.3020 - acc: 0.1086 - val_loss: 2.3029 - val_acc: 0.0672\n",
      "Epoch 8/120\n",
      "1206/1206 [==============================] - 0s 243us/step - loss: 2.3022 - acc: 0.0954 - val_loss: 2.3029 - val_acc: 0.0672\n",
      "Epoch 9/120\n",
      "1206/1206 [==============================] - 0s 248us/step - loss: 2.3019 - acc: 0.0970 - val_loss: 2.3030 - val_acc: 0.0672\n",
      "Epoch 10/120\n",
      "1206/1206 [==============================] - 0s 247us/step - loss: 2.3020 - acc: 0.1111 - val_loss: 2.3030 - val_acc: 0.0672\n",
      "Epoch 11/120\n",
      "1206/1206 [==============================] - 0s 287us/step - loss: 2.3018 - acc: 0.1053 - val_loss: 2.3031 - val_acc: 0.0672\n",
      "Epoch 12/120\n",
      "1206/1206 [==============================] - 0s 269us/step - loss: 2.3021 - acc: 0.1078 - val_loss: 2.3032 - val_acc: 0.0672\n",
      "Epoch 13/120\n",
      "1206/1206 [==============================] - 0s 264us/step - loss: 2.3019 - acc: 0.1078 - val_loss: 2.3032 - val_acc: 0.0672\n",
      "Epoch 14/120\n",
      "1206/1206 [==============================] - 0s 245us/step - loss: 2.3015 - acc: 0.1070 - val_loss: 2.3032 - val_acc: 0.0672\n",
      "Epoch 15/120\n",
      "1206/1206 [==============================] - 0s 245us/step - loss: 2.3018 - acc: 0.0954 - val_loss: 2.3033 - val_acc: 0.0672\n",
      "Epoch 16/120\n",
      "1206/1206 [==============================] - 0s 238us/step - loss: 2.3020 - acc: 0.1086 - val_loss: 2.3034 - val_acc: 0.0672\n",
      "Epoch 17/120\n",
      "1206/1206 [==============================] - 0s 247us/step - loss: 2.3019 - acc: 0.1078 - val_loss: 2.3035 - val_acc: 0.0672\n",
      "Epoch 18/120\n",
      "1206/1206 [==============================] - 0s 247us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3036 - val_acc: 0.0672\n",
      "Epoch 19/120\n",
      "1206/1206 [==============================] - 0s 242us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3037 - val_acc: 0.0672\n",
      "Epoch 20/120\n",
      "1206/1206 [==============================] - 0s 270us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3038 - val_acc: 0.0672\n",
      "Epoch 21/120\n",
      "1206/1206 [==============================] - 0s 246us/step - loss: 2.3019 - acc: 0.1078 - val_loss: 2.3039 - val_acc: 0.0672\n",
      "Epoch 22/120\n",
      "1206/1206 [==============================] - 0s 276us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3038 - val_acc: 0.0672\n",
      "Epoch 23/120\n",
      "1206/1206 [==============================] - 0s 248us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3038 - val_acc: 0.0672\n",
      "Epoch 24/120\n",
      "1206/1206 [==============================] - 0s 258us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3039 - val_acc: 0.0672\n",
      "Epoch 25/120\n",
      "1206/1206 [==============================] - 0s 276us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3041 - val_acc: 0.0672\n",
      "Epoch 26/120\n",
      "1206/1206 [==============================] - 0s 260us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3040 - val_acc: 0.0672\n",
      "Epoch 27/120\n",
      "1206/1206 [==============================] - 0s 287us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3041 - val_acc: 0.0672\n",
      "Epoch 28/120\n",
      "1206/1206 [==============================] - 0s 249us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3041 - val_acc: 0.0672\n",
      "Epoch 29/120\n",
      "1206/1206 [==============================] - 0s 248us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3043 - val_acc: 0.0672\n",
      "Epoch 30/120\n",
      "1206/1206 [==============================] - 0s 237us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3043 - val_acc: 0.0672\n",
      "Epoch 31/120\n",
      "1206/1206 [==============================] - 0s 247us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3043 - val_acc: 0.0672\n",
      "Epoch 32/120\n",
      "1206/1206 [==============================] - 0s 247us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3044 - val_acc: 0.0672\n",
      "Epoch 33/120\n",
      "1206/1206 [==============================] - 0s 242us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3043 - val_acc: 0.0672\n",
      "Epoch 34/120\n",
      "1206/1206 [==============================] - 0s 244us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3045 - val_acc: 0.0672\n",
      "Epoch 35/120\n",
      "1206/1206 [==============================] - 0s 252us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3047 - val_acc: 0.0672\n",
      "Epoch 36/120\n",
      "1206/1206 [==============================] - 0s 241us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3046 - val_acc: 0.0672\n",
      "Epoch 37/120\n",
      "1206/1206 [==============================] - 0s 240us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3048 - val_acc: 0.0672\n",
      "Epoch 38/120\n",
      "1206/1206 [==============================] - 0s 244us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3047 - val_acc: 0.0672\n",
      "Epoch 39/120\n",
      "1206/1206 [==============================] - 0s 244us/step - loss: 2.3019 - acc: 0.1078 - val_loss: 2.3049 - val_acc: 0.0672\n",
      "Epoch 40/120\n",
      "1206/1206 [==============================] - 0s 255us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3049 - val_acc: 0.0672\n",
      "Epoch 41/120\n",
      "1206/1206 [==============================] - 0s 246us/step - loss: 2.3018 - acc: 0.1078 - val_loss: 2.3049 - val_acc: 0.0672\n",
      "Epoch 42/120\n",
      "1206/1206 [==============================] - 0s 244us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3050 - val_acc: 0.0672\n",
      "Epoch 43/120\n",
      "1206/1206 [==============================] - 0s 248us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 44/120\n",
      "1206/1206 [==============================] - 0s 257us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 45/120\n",
      "1206/1206 [==============================] - 0s 283us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3050 - val_acc: 0.0672\n",
      "Epoch 46/120\n",
      "1206/1206 [==============================] - 0s 265us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 47/120\n",
      "1206/1206 [==============================] - 0s 274us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3050 - val_acc: 0.0672\n",
      "Epoch 48/120\n",
      "1206/1206 [==============================] - 0s 287us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 49/120\n",
      "1206/1206 [==============================] - 0s 263us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 50/120\n",
      "1206/1206 [==============================] - 0s 272us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3050 - val_acc: 0.0672\n",
      "Epoch 51/120\n",
      "1206/1206 [==============================] - 0s 285us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 52/120\n",
      "1206/1206 [==============================] - 0s 266us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 53/120\n",
      "1206/1206 [==============================] - 0s 262us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 54/120\n",
      "1206/1206 [==============================] - 0s 237us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 55/120\n",
      "1206/1206 [==============================] - 0s 310us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 56/120\n",
      "1206/1206 [==============================] - 0s 249us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 57/120\n",
      "1206/1206 [==============================] - 0s 235us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 58/120\n",
      "1206/1206 [==============================] - 0s 251us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206/1206 [==============================] - 0s 236us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 60/120\n",
      "1206/1206 [==============================] - 0s 238us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 61/120\n",
      "1206/1206 [==============================] - 0s 255us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 62/120\n",
      "1206/1206 [==============================] - 0s 241us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 63/120\n",
      "1206/1206 [==============================] - 0s 244us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 64/120\n",
      "1206/1206 [==============================] - 0s 233us/step - loss: 2.3010 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 65/120\n",
      "1206/1206 [==============================] - 0s 237us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 66/120\n",
      "1206/1206 [==============================] - 0s 295us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 67/120\n",
      "1206/1206 [==============================] - 0s 255us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 68/120\n",
      "1206/1206 [==============================] - 0s 249us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 69/120\n",
      "1206/1206 [==============================] - 0s 249us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 70/120\n",
      "1206/1206 [==============================] - 0s 270us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 71/120\n",
      "1206/1206 [==============================] - 0s 256us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3050 - val_acc: 0.0672\n",
      "Epoch 72/120\n",
      "1206/1206 [==============================] - 0s 252us/step - loss: 2.3009 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 73/120\n",
      "1206/1206 [==============================] - 0s 278us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3051 - val_acc: 0.0672\n",
      "Epoch 74/120\n",
      "1206/1206 [==============================] - 0s 280us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 75/120\n",
      "1206/1206 [==============================] - 0s 241us/step - loss: 2.3010 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 76/120\n",
      "1206/1206 [==============================] - 0s 248us/step - loss: 2.3018 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 77/120\n",
      "1206/1206 [==============================] - 0s 282us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3052 - val_acc: 0.0672\n",
      "Epoch 78/120\n",
      "1206/1206 [==============================] - 0s 252us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 79/120\n",
      "1206/1206 [==============================] - 0s 245us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 80/120\n",
      "1206/1206 [==============================] - 0s 240us/step - loss: 2.3018 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 81/120\n",
      "1206/1206 [==============================] - 0s 246us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 82/120\n",
      "1206/1206 [==============================] - 0s 281us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 83/120\n",
      "1206/1206 [==============================] - 0s 245us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 84/120\n",
      "1206/1206 [==============================] - 0s 288us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 85/120\n",
      "1206/1206 [==============================] - 0s 286us/step - loss: 2.3010 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 86/120\n",
      "1206/1206 [==============================] - 0s 295us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 87/120\n",
      "1206/1206 [==============================] - 0s 293us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 88/120\n",
      "1206/1206 [==============================] - 0s 286us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 89/120\n",
      "1206/1206 [==============================] - 0s 303us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 90/120\n",
      "1206/1206 [==============================] - 0s 280us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 91/120\n",
      "1206/1206 [==============================] - 0s 287us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3053 - val_acc: 0.0672\n",
      "Epoch 92/120\n",
      "1206/1206 [==============================] - 0s 274us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 93/120\n",
      "1206/1206 [==============================] - 0s 280us/step - loss: 2.3018 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 94/120\n",
      "1206/1206 [==============================] - 0s 296us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 95/120\n",
      "1206/1206 [==============================] - 0s 282us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 96/120\n",
      "1206/1206 [==============================] - 0s 253us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 97/120\n",
      "1206/1206 [==============================] - 0s 246us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 98/120\n",
      "1206/1206 [==============================] - 0s 239us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 99/120\n",
      "1206/1206 [==============================] - 0s 241us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3054 - val_acc: 0.0672\n",
      "Epoch 100/120\n",
      "1206/1206 [==============================] - 0s 252us/step - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 101/120\n",
      "1206/1206 [==============================] - 0s 253us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 102/120\n",
      "1206/1206 [==============================] - 0s 272us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 103/120\n",
      "1206/1206 [==============================] - 0s 236us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3057 - val_acc: 0.0672\n",
      "Epoch 104/120\n",
      "1206/1206 [==============================] - 0s 242us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 105/120\n",
      "1206/1206 [==============================] - 0s 235us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 106/120\n",
      "1206/1206 [==============================] - 0s 239us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3058 - val_acc: 0.0672\n",
      "Epoch 107/120\n",
      "1206/1206 [==============================] - 0s 245us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 108/120\n",
      "1206/1206 [==============================] - 0s 239us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3058 - val_acc: 0.0672\n",
      "Epoch 109/120\n",
      "1206/1206 [==============================] - 0s 250us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3057 - val_acc: 0.0672\n",
      "Epoch 110/120\n",
      "1206/1206 [==============================] - 0s 241us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3057 - val_acc: 0.0672\n",
      "Epoch 111/120\n",
      "1206/1206 [==============================] - 0s 255us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3057 - val_acc: 0.0672\n",
      "Epoch 112/120\n",
      "1206/1206 [==============================] - 0s 270us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3057 - val_acc: 0.0672\n",
      "Epoch 113/120\n",
      "1206/1206 [==============================] - 0s 245us/step - loss: 2.3012 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 114/120\n",
      "1206/1206 [==============================] - 0s 300us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 115/120\n",
      "1206/1206 [==============================] - 0s 262us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 116/120\n",
      "1206/1206 [==============================] - 0s 255us/step - loss: 2.3016 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 117/120\n",
      "1206/1206 [==============================] - 0s 238us/step - loss: 2.3013 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 118/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206/1206 [==============================] - 0s 253us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3055 - val_acc: 0.0672\n",
      "Epoch 119/120\n",
      "1206/1206 [==============================] - 0s 237us/step - loss: 2.3015 - acc: 0.1078 - val_loss: 2.3056 - val_acc: 0.0672\n",
      "Epoch 120/120\n",
      "1206/1206 [==============================] - 0s 245us/step - loss: 2.3014 - acc: 0.1078 - val_loss: 2.3057 - val_acc: 0.0672\n",
      "Test loss: 2.308111806349321\n",
      "Test accuracy: 0.09242424244681995\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# \n",
    "model = Sequential()\n",
    "# model.add(Conv2D(6, kernel_size=(3, 3),\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape))\n",
    "# model.add(Conv2D(12, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', input_shape=input_shape))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0985565 , 0.10572608, 0.09942745, 0.097695  , 0.10432712,\n",
       "       0.10236169, 0.09227589, 0.10220782, 0.1024944 , 0.09492807],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [1==i for i in Y]\n",
    "Y[temp].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 62, 62, 6)         222       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 60, 60, 12)        660       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 30, 30, 12)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 30, 30, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 10800)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               1382528   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,394,076\n",
      "Trainable params: 1,394,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0985565 , 0.10572608, 0.09942745, ..., 0.10220782, 0.1024944 ,\n",
       "        0.09492807],\n",
       "       [0.0985565 , 0.10572608, 0.09942745, ..., 0.10220782, 0.1024944 ,\n",
       "        0.09492807],\n",
       "       [0.0985565 , 0.10572608, 0.09942745, ..., 0.10220782, 0.1024944 ,\n",
       "        0.09492807],\n",
       "       ...,\n",
       "       [0.0985565 , 0.10572608, 0.09942745, ..., 0.10220782, 0.1024944 ,\n",
       "        0.09492807],\n",
       "       [0.0985565 , 0.10572608, 0.09942745, ..., 0.10220782, 0.1024944 ,\n",
       "        0.09492807],\n",
       "       [0.0985565 , 0.10572608, 0.09942745, ..., 0.10220782, 0.1024944 ,\n",
       "        0.09492807]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
